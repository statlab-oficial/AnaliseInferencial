# Estimação Pontual


## Estimador de Momentos

Este método de estimação é um dos métodos mais antigos de estimação. Ele é utilizado desde o século XVIII. A ideia é bem simples. Vamos inicialmente definir para o caso uniparamétrico:


Seja $X$ uma variável aleatória com a seguinte f.d.p. ou f.p. , $f(x\;|\theta)$, suporte $A$ , espaço paramétrico $\Theta$ e $E_{\theta}(X^r)<\infty,\; r=1,2,\dots$.

Seja a amostra aleatória $X_1,X_2,\ldots,X_n$ de $X$ . Vamos igualar o primeiro populacional  ao primeiro momento amostral, isto é,

$$E_{\theta}(X)= \bar{X}.$$ 


Vamos fazer alguns exemplos para fixar o conceito calculando também sua variância amostral.


**Exemplo 1:** Considere $X \sim U( (0,\theta)$ com


$$E_{\theta}(X)=\frac{\theta}{2}\quad \text{e} \quad V(X)=\frac{\theta^2}{12}.$$

Assim,


$$E_{\theta}(X)= \frac{\theta}{2}=\bar{X}.$$


O valor de $\theta$ é:


$$\theta=2\bar{X}.$$


Assim o estimador pelo m?todo dos momentos de $\theta$ é dado por:



$$\widehat{\theta}=T_1=2\bar{X}.$$


Note que:


$$E(\bar{X})= \mu = \frac{\theta}{2}.$$


Assim


$$E(T_1)=E(2\bar{X})=2\;E(\bar{X})=2\;\frac{\theta}{2}=\theta,$$
$T_1$ é um estimador não viciado de $\theta$.



Note que:


$$Var(\bar{X})=\frac{ \frac{\theta^2}{12}}{n}=\frac{\theta^2}{12n}.$$


Assim,


$$Var(T_1)=Var(2\bar{X})=4\;Var(\bar{X})=4\;\frac{\theta^2}{12n}=\frac{\theta^2}{3n}.$$

 
**Exemplo 2:**  Considere $X \sim U_d( A),\;\;A=\{1,2,\ldots,\theta\}$


O espaço paramétrico é dado por:


$$\Theta=\{1,2,\ldots\}.$$


Sabemos que:


$$E_{\theta}(X)=\frac{1+\theta}{2}\;\;\;;\; V(X)=\frac{\theta^2 -1}{12}.$$



Assim,


$$E_{\theta}(X)= \frac{1+\theta}{2}=\bar{X}.$$


O valor de $\theta$ é:


$$\theta=2\bar{X} -1.$$


Assim o estimador pelo m?todo dos momentos de $\theta$ é dado por:


$$\widehat{\theta}=T_2=2\bar{X}- 1.$$


Note que:


$$E(\bar{X})=\mu=\frac{1+\theta}{2}.$$


Assim


$$E(T_2)=E(2\bar{X} -1)=2\;E(\bar{X})-1=2\;\frac{1+\theta}{2}-1=\theta,$$

$T_2$ é um estimador não viciado de $\theta$.



Note que:


$$Var(\bar{X})=\frac{ \frac{\theta^2 -1}{12}}{n}=\frac{\theta^2 -1}{12n}.$$


Assim


$$Var(T_2)=Var(2\bar{X}-1)=4\;Var(\bar{X})=4\;\frac{\theta^2-1}{12n}=\frac{\theta^2-1}{3n}.$$


**Exemplo 3:**  Seja $X \sim Normal (\mu, 9).$

Sabemos que:


$$A=(-\infty,\infty)\;\;;\;\; \Theta= (-\infty,\infty).$$


Além disso:


$$E_{\mu}=\mu\;\;;\;\;V(X)=9.$$



Logo,


$$E_{\mu}=\mu=\bar{X}.$$


Assim


$$\widehat{\mu}=T_3=\bar{X}$$

é o estimador pelo método dos momentos para $\mu$.


Note que


$$E(T_3)= \mu\;\;\;\;Var(T_3)= \frac{9}{n}.$$



**Exemplo 4:**  Agora considere $X \sim Exponencial (\lambda).$


Sabemos que:


$$A=(0,\infty)\;\;;\;\; \Theta= (0,\infty).$$



Além disso:


$$E_{\lambda}=\frac{1}{\lambda}\;\;;\;\;V(X)=\frac{1}{\lambda^2}.$$


O estimador pelo método dos momentos é dado por:


$$E_{\lambda}=\frac{1}{\lambda}=\bar{X}.$$


Logo,


$$\widehat{\lambda}=T_4=\frac{1}{\bar{X}}.$$


Sabemos que

$$S \sim Gama(n,\lambda). $$


Note que:


$$M_S(t)=\left[ \frac{\lambda}{\lambda -t}\right]^n, \;t <\lambda.$$


A função geradora de momentos de $\bar{X}$ é dada por:


$$M_{\bar{X}}(t)=M_S(t/n)=\left[ \frac{\lambda}{\lambda -t/n}\right]^n, \;t/n <\lambda.$$


$$M_{\bar{X}}(t)=\left[ \frac{n\lambda}{n\lambda -t}\right]^n, \;t < n\lambda.$$



Assim


$$ V= \bar{X} \sim Gama(n,n\lambda).$$ 


A densidade de $V$ é dada por:


$$f_V(v)=\frac{(n\lambda)^n}{\Gamma(n)} v^{n-1}\;e^{-n\lambda v}\;\;I_A(v),\;A=(0,\infty).$$


Note que:


$$T_4=\frac{1}{\bar{X}}=\frac{1}{V}.$$


Logo


$$E(T_4)=\displaystyle \int_{0}^{\infty} \;\frac{1}{v}\;\frac{(n\lambda)^n}{\Gamma(n)} v^{n-1}\;e^{-n\lambda v}\;dv $$


$$E(T_4)= \frac{(n\lambda)^n}{\Gamma(n)}\;\displaystyle \int_{0}^{\infty}v^{(n-1)-1}\;e^{-n\lambda v}\;dv.$$



$$E(T_4)= \frac{(n\lambda)^n}{\Gamma(n)}\;IGG(a=n-1,b=n\lambda,c=1) ,$$

com $a=n-1>0$ ou $n>1$.



$$E(T_4)= \frac{(n\lambda)^n}{\Gamma(n)}\; \frac{\Gamma(n-1)}{(n\lambda)^{n-1}}= \frac {n\lambda}{n-1}=\frac{n}{n-1}\;\lambda \neq \lambda$$

Logo $T_4$ é um estimador viciado de $\lambda$. Mas ele é assintoticamente não viciado para $\lambda$.


Vamos calcular a variância de $T_4$:


$$E(T_4^2)=\displaystyle \int_{0}^{\infty} \;\frac{1}{v^2}\;\frac{(n\lambda)^n}{\Gamma(n)} v^{n-1}\;e^{-n\lambda v}\;dv $$


$$E(T_4^2 )= \frac{(n\lambda)^n}{\Gamma(n)}\;\displaystyle \int_{0}^{\infty}v^{(n-2)-1}\;e^{-n\lambda v}\;dv.$$


$$E(T_4^2)= \frac{(n\lambda)^n}{\Gamma(n)}\;IGG(a=n-2,b=n\lambda,c=1) ,$$

com $a=n-2>0$ ou $n>2$.


$$E(T_4^2 )= \frac{(n\lambda)^n}{\Gamma(n)}\; \frac{\Gamma(n-2)}{(n\lambda)^{n-2}}= \frac {n^2 \lambda^2}{(n-1)(n-2)}=\frac{n^2}{(n-1)(n-2)}\;\lambda^2$$

A variância de $T_4$ ? dada por:


$$Var(T_4)= \frac{n^2}{(n-1)(n-2)}\;\lambda^2- \frac{n^2}{(n-1)^2 }\;\lambda^2$$
 


$$Var(T_4)= \frac{n^2}{(n-1)^2(n-2)}\;\lambda^2 $$


Note que


$$\displaystyle \lim_{n\to \infty} V(T_4)=0,$$


assim $T_4$ é um estimador consistente para $\lambda$.  


O próximo exemplo mostrará uma situação distinta das anteriores.


**Exemplo 5:**  Seja $Y_i \sim Normal\left(\mu_i=\beta X_i, \sigma^2\right), i=1,2,\ldots,n,$\; independentes
com $X_i,i=1,2,\ldots,n$ constantes conhecidas.


Note que:


$$E(Y_i)=\beta X_i, i=1,2,\ldots,n.$$


Quando as variáveis não são identicamente distribuídas precisamos de  uma definição alternativa   para o estimador pelo método dos momentos:


$$\frac{\displaystyle \sum_{i=1}^{n} E(Y_i)}{n}=\bar{Y}.$$


Logo


$$\frac{\displaystyle \sum_{i=1}^{n} \beta X_i}{n}=\bar{Y}.$$

$$ \beta\; \;\frac{\displaystyle \sum_{i=1}^{n} X_i}{n}=\bar{Y}.$$

$$\beta\bar{X}=\bar{Y}.$$


$$\widehat{\beta}=\frac{\bar{Y}}{\bar{X}}= \frac{1}{n\bar{X}}\;\displaystyle \sum_{i=1}^{n} Y_i.$$


O estimador pelo método dos momentos é uma combinação linear de $Y_1,Y_2,\ldots,Y_n$.


Vamos mostrar que ele é não viciado.



$$E\left(\widehat{\beta} \right)=\;\frac{1}{n\bar{X}}\;\displaystyle \sum_{i=1}^{n} E(Y_i)=\;\frac{1}{n\bar{X}}\;\displaystyle \sum_{i=1}^{n}\;\beta X_i.$$


$$E\left(\widehat{\beta} \right)=\;\beta\;\;\frac{1}{n\bar{X}}\;\displaystyle \sum_{i=1}^{n}\; X_i=\beta.$$


Vamos calcular agora a variância:


$$Var\left(\widehat{\beta} \right)=\;\frac{1}{n^2\;\bar{X}^2}\;\displaystyle \sum_{i=1}^{n} V(Y_i)=\;\frac{1}{n^2\;\bar{X}^2 }\;n\;\sigma^2=\frac{\sigma^2}{n\;\bar{X}^2 }.$$



**Exemplo 6** Seja $X \sim N(0,\sigma^2)$. Qual o estimador $T$ pelo método dos momentos para $\sigma^2$?


Note que:


$$E(X)=0\;\;E(X^2)= \sigma^2.$$


$$T= \frac{\displaystyle \sum_{i=1}^{n}\; X_i^2}{n}.$$
 
 
Note que:


$$E(T)=\sigma^2.$$

$$Var(T)=\frac{1}{n^2} \;\displaystyle \sum_{i=1}^{n}\; V(X_i^2)= \frac{n V(X^2)}{n^2}=\frac{V(X^2)}{n}. $$


Mas

$$Var(X^2)=E(X^4)-E^2(X^2).$$


Sabemos que


$$Z^4=\left[\frac{X-\mu}{\sigma} \right]^4= \frac{X^4}{\sigma^4}.$$


A curtose da normal vale 3 logo:


$$E(Z^4)=3.$$


$$E(X^4)=3 \sigma^4.$$


$$V(X^2)=3 \sigma^4- \sigma^4=2 \sigma^4.$$


Logo,


$$Var(T)= \frac{2 \sigma^4}{n}.$$



Agora vamos estudar o caso biparamétrico:


Seja $X \sim N(\mu, \sigma^2)$. Calcule os estimadores pelo método dos momentos para $\mu$ e $\sigma^2$.


Como

$$E(X)=\mu\;\;\;\;\text{e}\;\;\;E(X^2)=\sigma^2 +\mu^2.$$

Assim vamos igualar os dois primeiros momentos populacionais aos dois primeiros momentos amostrais:


$$E(X)=\mu=\bar{X}.$$

$$\widehat{\mu}=\bar{X}.$$



$$E(X^2)=\sigma^2 +\mu^2= \frac{\displaystyle \sum_{i=1}^{n}\; X_i^2}{n}.$$
 
 
 Logo,
 
 
 $$\widehat{\sigma^2} +\hat{\mu^2}= \frac{\displaystyle \sum_{i=1}^{n}\; X_i^2}{n}.$$
 
 $$\widehat{\sigma^2}=\frac{\displaystyle \sum_{i=1}^{n}\; X_i^2}{n} -\bar{X}^2 =\frac{\displaystyle \sum_{i=1}^{n}\; X_i^2- n\;\bar{X}^2}{n}$$
 
 
 $$\widehat{\sigma^2}=\frac{\displaystyle \sum_{i=1}^{n}\; \left(X_i-\bar{X}\right)^2}{n}=\frac{(n-1)S^2}{n}. $$
 
 
 Note que 
 
 
 $$ E\left(\widehat{\sigma^2}\right)= \frac{n-1}{n} E(S^2)=\frac{n-1}{n}\;\sigma^2,$$
 
 que é viciado mas assintoticamente não viciado.
 
 
 Por outro lado temos:
 
 
 $$ Var\left(\widehat{\sigma^2}\right)= \frac{(n-1)^2}{n^2} Var(S^2)=\frac{(n-1)^2}{n^2}\; \frac{2\sigma^4}{n-1}= \frac{2(n-1)}{n^2} \sigma^4.$$
 

Note que $\widehat{\sigma^2}$ é um estimador consistente para $\sigma^2$.



**Exemplo 7:** Seja $X \sim Gama(\alpha >0,\beta >0)$. A f.d.p. de $X$ é dada por:


$$f(x|\alpha,\beta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}\;x^{\alpha-1}\;e^{-\beta\;x}\;I_A(x),\;\;A=(0,\infty).$$

Note que 

$$E(X)=\frac{\alpha}{\beta}\;\;\;,V(X)= \frac{\alpha}{\beta^2} \;\;\;\;\;\;;\;E(X^2)=\frac{\alpha}{\beta^2} +\frac{\alpha^2}{\beta^2}.$$


Sejam $\widehat{\alpha}$ e $\widehat{\beta}$ os estimadores pelo método dos momentos:


Assim


$$\frac{\widehat{\alpha}}{\widehat{\beta}}=\bar{X} $$


Assim,


$$\widehat{\alpha}=\bar{X}\;\widehat{\beta}.$$

 
$$\frac{\widehat{\alpha}}{\widehat{\beta}^2} +\frac{\widehat{\alpha}^2}{\widehat{\beta}^2}=\frac{1}{n}\;\displaystyle \sum_{i=1}^{n}\; X_i^2. $$

Note que:

$$ \frac{\hat{\alpha}}{\widehat{\beta}}\;\frac{1} {\widehat{\beta}} +\frac{\hat{\alpha}^2}{\hat{\beta}^2}=\frac{1}{n}\;\displaystyle \sum_{i=1}^{n}\; X_i^2. $$

$$\bar{X}\;\frac{1} {\widehat{\beta}} +\bar{X}^2=\frac{1}{n}\;\displaystyle \sum_{i=1}^{n}\; X_i^2 $$

$$\bar{X}\;\frac{1} {\hat{\beta}}= \frac{\displaystyle \sum_{i=1}^{n}\; X_i^2 -n \bar{X}^2}{n}= \hat{\sigma^2}$$

logo,


$$\hat{\beta}=\frac{\bar{X}}{ \hat{\sigma^2}}.$$


$$\hat{\alpha}=\bar{X}\;\widehat{\beta}=\frac{\bar{X}^2}{ \widehat{\sigma^2}}.$$


Uma maneira mais rápida de achar este estimadores é notar que:


$$\frac{E(X)}{V(X)}= \beta$$


logo

$$ \hat{\beta}=\frac{\bar{X}}{\widehat{\sigma^2}}.$$

Note que podemos usar tantos os momentos em relação é origem como os centrais. 

 